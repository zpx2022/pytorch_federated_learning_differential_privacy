[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)
# Combination of Federated Learning Baseline Algorithm and Laplace Mechanism-based Differential Privacy Technology

**Project Descriptionï¼š** This project is a fork of [rruisong/pytorch_federated_learning](https://github.com/rruisong/pytorch_federated_learning). I would like to express my gratitude to the original author for his basic work.

## âœ¨ Core Features

Compared with the parent project, this project implements privacy protection based on the Laplace mechanism and makes a series of architectural optimizations:
- **Support for Local Differential Privacy (LDP)**: The LDP mechanism is integrated, and gradient clipping and Laplacian noise can be added by configuration to study federated learning under privacy protection.

- **Efficient parallel simulation**: The client's **parallel training** is implemented using `concurrent.futures`, which can make full use of multi-core CPU resources and significantly shorten the simulation time.

- **Early Termination**: Supports the Early Stopping mechanism, which automatically stops training when the model performance no longer improves within the set patience value, saving computing resources.

- **Breakpoint Resume**: It can automatically save and load training checkpoints (`checkpoint`), making it easy to resume long experiments from interruptions.

## âš™ï¸ Project structure

```text
â”œâ”€â”€ config/                   # Storage of configuration files
â”‚   â””â”€â”€ test_config.yaml      # Main experimental configuration files
â”œâ”€â”€ fed_baselines/            # Core algorithm implementation
â”‚   â”œâ”€â”€ client_base.py        # Client base class (implementation of FedAvg)
â”‚   â”œâ”€â”€ client_fedprox.py     # FedProx algorithm client
â”‚   â”œâ”€â”€ client_scaffold.py    # SCAFFOLD algorithm client
â”‚   â”œâ”€â”€ client_fednova.py     # FedNova client
â”‚   â”œâ”€â”€ server_base.py        # Server base class (implementation of FedAvg)
â”‚   â”œâ”€â”€ server_scaffold.py    # SCAFFOLD algorithm server
â”‚   â””â”€â”€ server_fednova.py     # FedNova algorithm server
â”œâ”€â”€ figures/                  # Storage of generated charts
â”œâ”€â”€ postprocessing/           # Result post-processing
â”‚   â”œâ”€â”€ eval_main.py          # Main program for result evaluation and visualization
â”‚   â””â”€â”€ recorder.py           # Result recording and drawing tool class
â”œâ”€â”€ preprocessing/            # Data preprocessing
â”‚   â””â”€â”€ baselines_dataloader.py # Data loading and Non-IID partitioning
â”œâ”€â”€ utils/                    # Auxiliary tools
â”‚   â”œâ”€â”€ models.py             # Neural network model definition
â”‚   â””â”€â”€ fed_utils.py          # Federated learning auxiliary function
â”œâ”€â”€ fl_main.py                # Federated learning main training program
â””â”€â”€ requirements.txt          # Python dependency package list
```

## ğŸš€ Quick Start

### 1. Environment preparation

It is recommended to use a virtual environment (such as `conda` or `venv`) to manage project dependencies.

```bash
# Clone the repository
git clone https://github.com/zpx2022/pytorch_federated_learning_differential_privacy.git
cd pytorch_federated_learning_differential_privacy

# (Optional, recommended) Create and activate the conda virtual environment
conda create -n fldp python=3.8
conda activate fldp

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure the experiment
Open the test_config.yaml file and modify the experimental parameters according to your needs.

### 3. Run training
Execute the main program fl_main.py to start training. All results and checkpoints will be saved in the results/ and checkpoints/ directories by default.
```bash
python fl_main.py --config test_config.yaml
```

### 4. Evaluate and visualize results
Draw all .json result files in the results/ directory into a graph
```bash
python eval_main.py --sys-res_root results
```
